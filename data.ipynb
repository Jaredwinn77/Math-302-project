{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e90666",
   "metadata": {},
   "source": [
    "This notebook is for the Math 302 data project.\n",
    "Created by Alice Lundgre, Gideon Hale, and Jared Winn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c42611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcb965",
   "metadata": {},
   "source": [
    "The first section of this notebook deals with loading and cleaning the data.\n",
    "We have chosen to us the US traffic accidents dataset, and downloaded it from Kaggle. \n",
    "To apply this to another dataset you must download a CSV version of your dataset and change the file paths. However, be aware that much of the cleaning and reformating we do here is not universal, we treat different features of the dataset differently, and often specify rule for cleaning by feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f640c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the path to desired dataset.\n",
    "path = \"US_Accidents_March23.csv\"\n",
    "# Note that the datset is very large, as such execution of this cell can take some time. \n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76886e72",
   "metadata": {},
   "source": [
    "Now that we have loaded the data we wish to study and clean it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b29a7c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7728394, 46)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>Start_Lng</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>End_Lng</th>\n",
       "      <th>Distance(mi)</th>\n",
       "      <th>...</th>\n",
       "      <th>Roundabout</th>\n",
       "      <th>Station</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Turning_Loop</th>\n",
       "      <th>Sunrise_Sunset</th>\n",
       "      <th>Civil_Twilight</th>\n",
       "      <th>Nautical_Twilight</th>\n",
       "      <th>Astronomical_Twilight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A-1</td>\n",
       "      <td>Source2</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-02-08 05:46:00</td>\n",
       "      <td>2016-02-08 11:00:00</td>\n",
       "      <td>39.865147</td>\n",
       "      <td>-84.058723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A-2</td>\n",
       "      <td>Source2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 06:07:59</td>\n",
       "      <td>2016-02-08 06:37:59</td>\n",
       "      <td>39.928059</td>\n",
       "      <td>-82.831184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A-3</td>\n",
       "      <td>Source2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 06:49:27</td>\n",
       "      <td>2016-02-08 07:19:27</td>\n",
       "      <td>39.063148</td>\n",
       "      <td>-84.032608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-4</td>\n",
       "      <td>Source2</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-02-08 07:23:34</td>\n",
       "      <td>2016-02-08 07:53:34</td>\n",
       "      <td>39.747753</td>\n",
       "      <td>-84.205582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-5</td>\n",
       "      <td>Source2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 07:39:07</td>\n",
       "      <td>2016-02-08 08:09:07</td>\n",
       "      <td>39.627781</td>\n",
       "      <td>-84.188354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID   Source  Severity           Start_Time             End_Time  \\\n",
       "0  A-1  Source2         3  2016-02-08 05:46:00  2016-02-08 11:00:00   \n",
       "1  A-2  Source2         2  2016-02-08 06:07:59  2016-02-08 06:37:59   \n",
       "2  A-3  Source2         2  2016-02-08 06:49:27  2016-02-08 07:19:27   \n",
       "3  A-4  Source2         3  2016-02-08 07:23:34  2016-02-08 07:53:34   \n",
       "4  A-5  Source2         2  2016-02-08 07:39:07  2016-02-08 08:09:07   \n",
       "\n",
       "   Start_Lat  Start_Lng  End_Lat  End_Lng  Distance(mi)  ... Roundabout  \\\n",
       "0  39.865147 -84.058723      NaN      NaN          0.01  ...      False   \n",
       "1  39.928059 -82.831184      NaN      NaN          0.01  ...      False   \n",
       "2  39.063148 -84.032608      NaN      NaN          0.01  ...      False   \n",
       "3  39.747753 -84.205582      NaN      NaN          0.01  ...      False   \n",
       "4  39.627781 -84.188354      NaN      NaN          0.01  ...      False   \n",
       "\n",
       "  Station   Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset  \\\n",
       "0   False  False           False          False        False          Night   \n",
       "1   False  False           False          False        False          Night   \n",
       "2   False  False           False           True        False          Night   \n",
       "3   False  False           False          False        False          Night   \n",
       "4   False  False           False           True        False            Day   \n",
       "\n",
       "  Civil_Twilight Nautical_Twilight Astronomical_Twilight  \n",
       "0          Night             Night                 Night  \n",
       "1          Night             Night                   Day  \n",
       "2          Night               Day                   Day  \n",
       "3            Day               Day                   Day  \n",
       "4            Day               Day                   Day  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425f03d",
   "metadata": {},
   "source": [
    "We find that the data has 7728394 rows and 46 columns. We can already see that some of the columns will need to be trimmed (ie ID and source) and that there are several NaN values which we will need to take care of.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c89302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the ID and source columns\n",
    "df = df.drop(columns=[\"ID\", \"Source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae97cb",
   "metadata": {},
   "source": [
    "Looking at the attributes summary on Kaggle we identify more columns to remove:\n",
    "* Description ~ Natural language not parsable by simple ML techniques\n",
    "* Country ~ All data is from the United States\n",
    "* Timezone ~ Redundent given we already have a precise lat-long position\n",
    "* Airport_Code ~ Redundent given we already have a precise lat-long position\n",
    "* Street ~ Does not provide information that can be easily used by a simple ML model\n",
    "\n",
    "\n",
    "\n",
    "We will keep several other columns, such as City, State, ect to determine how they correlate to accidents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73b7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Description\", \"Country\", \"Timezone\", \"Airport_Code\",\"Street\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8909e6",
   "metadata": {},
   "source": [
    "Now we want to get an idea of how much data is missing, specifically if there are any rows or columns that are almost completely empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d3e288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: missing 0.000 %\n",
      "Zipcode: missing 0.000 %\n",
      "Astronomical_Twilight: missing 0.003 %\n",
      "Civil_Twilight: missing 0.003 %\n",
      "Sunrise_Sunset: missing 0.003 %\n",
      "Nautical_Twilight: missing 0.003 %\n",
      "Weather_Timestamp: missing 0.016 %\n",
      "Pressure(in): missing 0.018 %\n",
      "Temperature(F): missing 0.021 %\n",
      "Weather_Condition: missing 0.022 %\n",
      "Humidity(%): missing 0.023 %\n",
      "Wind_Direction: missing 0.023 %\n",
      "Visibility(mi): missing 0.023 %\n",
      "Wind_Speed(mph): missing 0.074 %\n",
      "Wind_Chill(F): missing 0.259 %\n",
      "Precipitation(in): missing 0.285 %\n",
      "End_Lng: missing 0.440 %\n",
      "End_Lat: missing 0.440 %\n"
     ]
    }
   ],
   "source": [
    "rows, cols = df.shape\n",
    "missing = df.isna()\n",
    "counts = missing.sum()\n",
    "for col, cnt in counts[counts > 0].sort_values().items():\n",
    "    pct = (cnt / rows)\n",
    "    print(f\"{col}: missing {pct:.3f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f9e9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3562877, 39)\n"
     ]
    }
   ],
   "source": [
    "# What would happen if we drop every row with Nan values?\n",
    "test = df.dropna()\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69cc127",
   "metadata": {},
   "source": [
    "We identify the following columns as potential problems:\n",
    "* Wind Chill\n",
    "* Precipitation\n",
    "* End_Lng\n",
    "* End_lat\n",
    "\n",
    "Even though we could drop every entry with NaN values and still have a sizable dataset, for the sake of this assingment we will find other ways to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52cc6b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    5.729375e+06\n",
      "mean     5.825105e+01\n",
      "std      2.238983e+01\n",
      "min     -8.900000e+01\n",
      "25%      4.300000e+01\n",
      "50%      6.200000e+01\n",
      "75%      7.500000e+01\n",
      "max      2.070000e+02\n",
      "Name: Wind_Chill(F), dtype: float64\n",
      "4429402\n"
     ]
    }
   ],
   "source": [
    "# Inspect values of Wind_Chill\n",
    "print(df['Wind_Chill(F)'].describe())\n",
    "\n",
    "# Let's see how many entries match Temperature\n",
    "print((df['Wind_Chill(F)'] == df['Temperature(F)']).sum())\n",
    "\n",
    "# It seems reasonable to replace NaN Wind_Chill values with the corrosponding value in Temperature. \n",
    "df['Wind_Chill(F)'] = df['Wind_Chill(F)'].fillna(df['Temperature(F)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24370efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    5.524808e+06\n",
      "mean     8.407210e-03\n",
      "std      1.102246e-01\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      3.647000e+01\n",
      "Name: Precipitation(in), dtype: float64\n",
      "4991718\n"
     ]
    }
   ],
   "source": [
    "# Inspect values of Precipitation \n",
    "print(df['Precipitation(in)'].describe())\n",
    "\n",
    "# Let's see how many entries are zero\n",
    "print((df['Precipitation(in)'] == 0).sum())\n",
    "\n",
    "# It seems pretty safe to assume that NaN values of preciptation are 0\n",
    "df['Precipitation(in)'] = df['Precipitation(in)'].fillna(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac604c",
   "metadata": {},
   "source": [
    "We now only have the ending latitude and longitude to deal with. Since the distance of the effect on traffic is already recorded, and we have the initial point, and a large portion of the dataset has Nan Values in these columns, it makes sense to delete them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24d9e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = [\"End_Lng\", \"End_Lat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a214a",
   "metadata": {},
   "source": [
    "Now we can clean up the rest of the NaNs without loosing too much of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac2ec81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c7df9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3498896\n"
     ]
    }
   ],
   "source": [
    "# Number of entries saved through our data cleaning\n",
    "print(df.shape[0] - test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5096bb",
   "metadata": {},
   "source": [
    "Next we move on to checking if the data is in the correct format, if there are duplicates, and if all the values fall in the correct ranges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e25b3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7061773 entries, 2 to 7728393\n",
      "Data columns (total 37 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   Severity               int64  \n",
      " 1   Start_Time             object \n",
      " 2   End_Time               object \n",
      " 3   Start_Lat              float64\n",
      " 4   Start_Lng              float64\n",
      " 5   Distance(mi)           float64\n",
      " 6   City                   object \n",
      " 7   County                 object \n",
      " 8   State                  object \n",
      " 9   Zipcode                object \n",
      " 10  Weather_Timestamp      object \n",
      " 11  Temperature(F)         float64\n",
      " 12  Wind_Chill(F)          float64\n",
      " 13  Humidity(%)            float64\n",
      " 14  Pressure(in)           float64\n",
      " 15  Visibility(mi)         float64\n",
      " 16  Wind_Direction         object \n",
      " 17  Wind_Speed(mph)        float64\n",
      " 18  Precipitation(in)      float64\n",
      " 19  Weather_Condition      object \n",
      " 20  Amenity                bool   \n",
      " 21  Bump                   bool   \n",
      " 22  Crossing               bool   \n",
      " 23  Give_Way               bool   \n",
      " 24  Junction               bool   \n",
      " 25  No_Exit                bool   \n",
      " 26  Railway                bool   \n",
      " 27  Roundabout             bool   \n",
      " 28  Station                bool   \n",
      " 29  Stop                   bool   \n",
      " 30  Traffic_Calming        bool   \n",
      " 31  Traffic_Signal         bool   \n",
      " 32  Turning_Loop           bool   \n",
      " 33  Sunrise_Sunset         object \n",
      " 34  Civil_Twilight         object \n",
      " 35  Nautical_Twilight      object \n",
      " 36  Astronomical_Twilight  object \n",
      "dtypes: bool(13), float64(10), int64(1), object(13)\n",
      "memory usage: 1.4+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676babd",
   "metadata": {},
   "source": [
    "We see that the time columns are objects, but we need to check that they are date time objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55db1c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    2016-02-08 06:49:27\n",
       "3    2016-02-08 07:23:34\n",
       "4    2016-02-08 07:39:07\n",
       "5    2016-02-08 07:44:26\n",
       "6    2016-02-08 07:59:35\n",
       "Name: Start_Time, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Start_Time'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "682baef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    2016-02-08 07:19:27\n",
       "3    2016-02-08 07:53:34\n",
       "4    2016-02-08 08:09:07\n",
       "5    2016-02-08 08:14:26\n",
       "6    2016-02-08 08:29:35\n",
       "Name: End_Time, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['End_Time'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8d113c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    2016-02-08 06:56:00\n",
       "3    2016-02-08 07:38:00\n",
       "4    2016-02-08 07:53:00\n",
       "5    2016-02-08 07:51:00\n",
       "6    2016-02-08 07:56:00\n",
       "Name: Weather_Timestamp, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Weather_Timestamp'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d6b85",
   "metadata": {},
   "source": [
    "So we need to convert to a date time object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8c1c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to date time object\n",
    "df['Start_Time'] = df['Start_Time'].str.replace(r'\\.\\d+', '', regex=True)\n",
    "df['End_Time']   = df['End_Time'].str.replace(r'\\.\\d+', '', regex=True)\n",
    "df['Weather_Timestamp']   = df['Weather_Timestamp'].str.replace(r'\\.\\d+', '', regex=True)\n",
    "\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'])\n",
    "df['End_Time']   = pd.to_datetime(df['End_Time'])\n",
    "df['Weather_Timestamp'] = pd.to_datetime(df['Weather_Timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3705564",
   "metadata": {},
   "source": [
    "Next we will check if there are duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f953cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132349\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78417b5",
   "metadata": {},
   "source": [
    "This is quite a lot, we checked, and before any cleaning was done there were 0 duplicate entries, after removing the ID and source there were 102338, and after removing the description column there were 121499. \n",
    "\n",
    "Given these numbers, it seems likely that the three seperate sources often recorded the same accident, with differences in the source and description column accounting for almost all of the duplicates. As such we will drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "662363f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9978b",
   "metadata": {},
   "source": [
    "Next we will check that the entries in each column make sense.\n",
    "Some googling reveals the following facts:\n",
    "* The min and max recorded temps in the continental US are -70 F, 134 F\n",
    "* The min windchill recorded in US is -108 F\n",
    "* The highest windspeed between 2016 and 2023 was 165 mph\n",
    "\n",
    "While it is possible that values under these limits are also errors, we can remove anything outside of these ranges without any trepidation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "526ee5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Temperature(F)'] >= -70) & (df['Temperature(F)'] <= 134)]\n",
    "df = df[(df['Wind_Chill(F)'] >= -108) & (df['Wind_Chill(F)'] <= 134)]\n",
    "df = df[(df['Wind_Speed(mph)'] <= 165)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6fd3bf",
   "metadata": {},
   "source": [
    "As we perform more analysis we my further clean the data, or for example create new combinations such as the amount of time that traffic was effected, or the time between the weather report and the accident, but for now this cleaning gives us what we need to start visualizing our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c14ab91",
   "metadata": {},
   "source": [
    "Since these steps take a considerable amount of time to run, we will save a cleaned version of the dataset that can be loaded without running the above cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea14a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "413e1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load the cleaned data checkpoint\n",
    "path = \"Cleaned.csv\"\n",
    "# Note that the datset is very large, as such execution of this cell can take some time. \n",
    "# But its much faster than the full version\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11834a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6929344, 37)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c3fcd",
   "metadata": {},
   "source": [
    "**Visualizations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c7a2e",
   "metadata": {},
   "source": [
    "Now that our data is clean, we are ready to create some visualizations in order to understand its structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c507a51",
   "metadata": {},
   "source": [
    "Geographic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f78dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure similar to the covid heat map we made in the lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f3a37",
   "metadata": {},
   "source": [
    "Temporal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78cb24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation of geographic distribution, but as time move forwards each accident that happened that day is marked. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c3c42",
   "metadata": {},
   "source": [
    "K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5604951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures for several values of k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c25e5d4",
   "metadata": {},
   "source": [
    "PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4192cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with various numbers of components "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4c820",
   "metadata": {},
   "source": [
    "Column statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fdf3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures for several columns showing histograms of values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0bf2f3",
   "metadata": {},
   "source": [
    "**Statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de155935",
   "metadata": {},
   "source": [
    "We have qualitatively analyzed our data with figures, now we will explore quantitative attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79282a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cells for analysis of mean, mode, range ect, ect, ect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6ea6a",
   "metadata": {},
   "source": [
    "**Model and Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3857a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cells for building models and making predictions "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
